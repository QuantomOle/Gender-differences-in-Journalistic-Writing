---
title: "classification_models2"
output: html_document
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#load libraries
library(tidyverse)
library(caret)
library(quanteda)
library(kernlab)
```
 
```{r}
#import data
bloomberg <- read.csv("bloomberg.csv", stringsAsFactors = FALSE)
df <- bloomberg %>% select(outlet, domain, text, gender)
remove(bloomberg)
```


```{r}
#split data into training and test set
set.seed(123)

index <- createDataPartition(df$gender, p =0.7, list = FALSE)

train <- df[index, ]
test <- df[-index, ]

#create variable to denote if observation is train or test
train$train_test <- c("train")
test$train_test <- c("test")

#merge train and test data
merged <- rbind(train, test)

#remove test and train
remove(train, test)

```

#Preprocessing and Feature Extraction

```{r}

#function to generate dfm
makeDFM <- function(df) {
  corpus <- corpus(df, text_field = "text")
  doctm <- dfm(corpus, remove=stopwords("english"), verbose=TRUE,
                ngrams = 1L,
                stem = TRUE,
     remove_punct=TRUE, 
     remove_numbers=TRUE,
     remove_symbols = TRUE)
  
  doctm <- dfm_trim(doctm, max_docfreq = 0.9, docfreq_type = "prop")
  
  return(doctm)
}

```

## Large dfm
```{r}
#create dfm
merged_dfm <- makeDFM(merged)

#convert dfm's to dataframes
merged_df_large <- convert(merged_dfm, to = "data.frame")

#drop document names: column 1
merged_df_large <- merged_df_large[,-1]

#split back into train and test sets
dfTrain_large <- merged_df_large[which(merged$train_test == "train"), ]

dfTest_large <- merged_df_large[which(merged$train_test == "test"), ]

#append article labels as last column
dfTrain_large$author_gender <- merged$gender[which(merged$train_test == "train")]
dfTest_large$author_gender <- merged$gender[which(merged$train_test == "test")]

```

##Feature Selection
The feature selection technique applied here was to select the topmost frequent 200 words for each gender, and then ignoring the common words which appear in both.  We then match the features of our initial `dfm` to these words resulting into a `subdfm`. As an extension, we create a tf-idf weighted version of the sub `dfm`.

```{r}
#create dfm
merged_dfm <- makeDFM(merged)

#get topmost frequent 200 words for each gender
topwords <- topfeatures(merged_dfm, 200, groups = "gender")

#convert list elements to dataframes
dtopwords <- lapply(topwords, function(x) {
  x <- as.data.frame(x)
  tibble::rownames_to_column(x)
} )
#extract only the unique words
utopwords <- unique(unlist(lapply(dtopwords, function(x) unique(x[,1]))))

#subset dfm
subdfm <- merged_dfm %>%
  dfm_select(pattern = utopwords)

#create tf-idf weighted version of dfm
subdfm_weighted <- dfm_tfidf(subdfm)

#convert dfm's to dataframes
merged_df <- convert(subdfm, to = "data.frame")
merged_df_weighted <- convert(subdfm_weighted, to = "data.frame")

#drop document names: column 1
merged_df <- merged_df[,-1]
merged_df_weighted <- merged_df_weighted[,-1]
```

```{r}
#split back into train and test sets
dfTrain <- merged_df[which(merged$train_test == "train"), ]
dfTrain_w <- merged_df_weighted[which(merged$train_test == "train"), ]

dfTest <- merged_df[which(merged$train_test == "test"), ]
dfTest_w <- merged_df_weighted[which(merged$train_test == "test"), ]

#remove objects that are no longer necessary
#remove(dtopwords, merged_dfm, merged_df, merged_df_weighted, subdfm, subdfm_weighted, topwords)

#append article labels as last column
dfTrain$author_gender <- merged$gender[which(merged$train_test == "train")]
dfTest$author_gender <- merged$gender[which(merged$train_test == "test")]
dfTrain_w$author_gender <- merged$gender[which(merged$train_test == "train")]
dfTest_w$author_gender <- merged$gender[which(merged$train_test == "test")]
```

#Model Training

##Naive Bayes

###Large dataset
```{r}
#Different pre-processing using dfm format instead of dataframe

train_dfm_large <- dfm_subset(merged_dfm, train_test=="train")
test_dfm_large <- dfm_subset(merged_dfm, train_test=="test")

```

```{r}
nb_model <- textmodel_nb(train_dfm_large, merged$gender[which(merged$train_test == "train")])

predict_test<- predict(nb_model, newdata = test_dfm_large)

table2 <- table(merged$gender[which(merged$train_test == "test")], predict_test)
confusionMatrix(table2)
```

###Subset
```{r, warning=FALSE, message=FALSE}
#resampling scheme
ctrl <- trainControl(method = "cv", number = 5)

#fit a Naive Bayes model using weighted Train set
set.seed(123)
nb.small <- train(author_gender ~., 
                 data = dfTrain, 
                 method="nb", 
                 trControl = ctrl)
#model output
nb.small

plot(nb.small)

# predict on test data
nb.small.predict <- predict(nb.small, newdata = dfTest)

#confusion matrix

confusionMatrix(nb.small.predict, as.factor(dfTest$author_gender))

#get execution time
nb.small$times
```


##Ridge regression

###Large dataset

```{r}

#fit Ridge Regression using unweighted Train set
set.seed(123)
ridge <- train(x = as.matrix(train_dfm_large),
                     y = factor(merged$gender[which(merged$train_test == "train")]),
                     method = "glmnet")

#model output 
ridge

# predict on test data
ridge.predict <- predict(ridge, newdata = test_dfm_large) 

#confusion matrix
confusionMatrix(ridge.predict, as.factor(merged$gender[which(merged$train_test == "test")]))

#get execution time
ridge$times

```


###Subset
```{r}
# resampling scheme
ctrl <- trainControl("cv", number = 5)

#set up a grid range of lambda values
lambda <- 10^seq(-3, 3, length = 100)

#fit Ridge Regression using unweighted Train set
set.seed(123)
ridge_small <- train(
  author_gender ~., data = dfTrain, method = "glmnet",
  trControl = ctrl,
  tuneGrid = expand.grid(alpha = 0, lambda = lambda)
  )

#model output 
ridge_small

# predict on test data
ridge_small.predict <- predict(ridge_small, newdata = dfTest) 

#confusion matrix
confusionMatrix(ridge_small.predict, as.factor(dfTest$author_gender))

#get execution time
ridge_small$times
```

##KNN
We use 5-fold cross validation to help reduce risk of overfitting in the models. 

### Large dataset

```{r}
#resampling scheme
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE)

#tuning parameter: K
set.seed(123)
knn.large<- train(x = as.matrix(train_dfm_large),
                     y = factor(merged$gender[which(merged$train_test == "train")]),
                  k=18,
                 method="knn")

#model output
knn.large

plot(knn.large) 

# predict on test data
knn.predict_large <- predict(knn.large, newdata = test_dfm_large)

#confusion matrix
confusionMatrix(knn.predic_large, as.factor(merged$gender[which(merged$train_test == "test")]))

#get execution time
knn.large$times
```

### Subset
```{r}
#resampling scheme
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE)

#tuning parameter: K
set.seed(123)
knn.fit <- train(author_gender ~., 
                 data = dfTrain, 
                 method="knn", 
                 trControl = ctrl,
                 tuneLength = 20)

#model output
knn.fit

plot(knn.fit) 

# predict on test data
knn.predict <- predict(knn.fit, newdata = dfTest)

#confusion matrix
confusionMatrix(knn.predict, as.factor(dfTest$author_gender))

#get execution time
knn.fit$times
```

## Random Forest 

###Large dataset
```{r}
#set up parallel processing
#install.packages("doParallel")
library(doParallel)
getDoParWorkers()
cores <- 6
registerDoParallel(cores = cores)

#mtry: Number of random variables collected at each split
mtry <- sqrt(ncol(dfTrain_large))
#ntree: Number of trees to grow.
ntree <- 3

#randomly generate 15 mtry values with tuneLength = 15
control <- trainControl(method='cv', 
                        number=5, 
                        search = 'random')

set.seed(123)
rf.large <- train(x = as.matrix(train_dfm_large),
                   y = factor(merged$gender[which(merged$train_test == "train")]),
                   method = 'rf',
                   metric = 'Accuracy',
                   num.trees = 200)

#output
rf.large

#plots
plot(rf.large)

#predict on test data
rf.large.predict <- predict(rf.large,newdata = test_dfm_large)

#confusion matrix
confusionMatrix(rf.large.predict, as.factor(merged$gender[which(merged$train_test == "test")]))

#compute time
rf.large$times
```

###Subset
```{r}
#set up parallel processing
#install.packages("doParallel")
library(doParallel)
getDoParWorkers()
cores <- 1
registerDoParallel(cores = cores)

#mtry: Number of random variables collected at each split
mtry <- sqrt(ncol(dfTrain))
#ntree: Number of trees to grow.
ntree <- 3

#randomly generate 15 mtry values with tuneLength = 15
control <- trainControl(method='cv', 
                        number=5, 
                        search = 'random')

set.seed(123)
rf.small <- train(author_gender ~ .,
                   data = dfTrain,
                   method = 'rf',
                   metric = 'Accuracy',
                   tuneLength  = 15, 
                   trControl = control)

#output
rf.small

#plots
plot(rf.small)

#predict on test data
rf.small.predict <- predict(rf.small,newdata = dfTest)

#confusion matrix
confusionMatrix(rf.small.predict, as.factor(dfTest$author_gender))

#compute time
rf.small$times
```

## Support Vector Machine

###Large dataset
```{r}
# resampling scheme
ctrl <- trainControl(method="cv", number = 5, classProbs = TRUE)

# fit SVM using the unweighted train set
# kernel: linear 
# tuning parameters: C 
set.seed(123)
svm.large  <- train(x = as.matrix(train_dfm_large),
                   y = factor(merged$gender[which(merged$train_test == "train")]), method = "svmLinear", cost=1)

#outputs
svm.large

# predict on test data
svm.large.predict <- predict(svm.large,newdata = dfTest_large)

#confusion matrices
confusionMatrix(svm.large.predict, as.factor(dfTest_large$author_gender))

# get execution time
svm.large$times
```

###Subset
```{r}
# resampling scheme
ctrl <- trainControl(method="cv", number = 5, classProbs = TRUE)

# fit SVM using the unweighted train set
# kernel: linear 
# tuning parameters: C 
set.seed(123)
svm.small  <- train(author_gender ~ ., data=dfTrain, trControl = ctrl, method = "svmLinear")

#outputs
svm.small

# predict on test data
svm.small.predict <- predict(svm.small,newdata = dfTest)

#confusion matrices
confusionMatrix(svm.small.predict, as.factor(dfTest$author_gender))

# get execution time
svm.small$times
```

