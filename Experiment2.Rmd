---
title: "Experiment2"
output: html_document
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#load libraries
library(tidyverse)
library(caret)
library(quanteda)
library(kernlab) #svm
library(klaR) #naive bayes
library(glmnet) 
```
 
```{r}
#import data
bloomberg <- read.csv("bloomberg.csv", stringsAsFactors = FALSE)
df <- bloomberg %>% dplyr::select(outlet, domain, text, gender)
remove(bloomberg)
```


```{r}
#split data into training and test set
set.seed(123)

index <- createDataPartition(df$gender, p =0.7, list = FALSE)

train <- df[index, ]
test <- df[-index, ]

#create variable to denote if observation is train or test
train$train_test <- c("train")
test$train_test <- c("test")

#merge train and test data
merged <- rbind(train, test)

#remove test and train
remove(train, test)

```

#Preprocessing and Feature Extraction

```{r}

#function to generate dfm
makeDFM <- function(df) {
  corpus <- corpus(df, text_field = "text")
  doctm <- dfm(corpus, remove=stopwords("english"), verbose=TRUE,
                ngrams = 1L,
                stem = TRUE,
     remove_punct=TRUE, 
     remove_numbers=TRUE,
     remove_symbols = TRUE)
  
  doctm <- dfm_trim(doctm, max_docfreq = 0.9, docfreq_type = "prop")
  
  return(doctm)
}

```

##Feature Selection
The feature selection technique applied here was to select the topmost frequent 500 words for each gender, and then ignoring the common words which appear in both.  We then match the features of our initial `dfm` to these words resulting into a `subdfm`. As an extension, we create a tf-idf weighted version of the sub `dfm`.

```{r}
#create dfm
merged_dfm <- makeDFM(merged)

#get topmost frequent 200 words for each gender
topwords <- topfeatures(merged_dfm, 200, groups = "gender")

#convert list elements to dataframes
dtopwords <- lapply(topwords, function(x) {
  x <- as.data.frame(x)
  tibble::rownames_to_column(x)
} )
#extract only the unique words
utopwords <- unique(unlist(lapply(dtopwords, function(x) unique(x[,1]))))

#subset dfm
subdfm <- merged_dfm %>%
  dfm_select(pattern = utopwords)

#create tf-idf weighted version of dfm
subdfm_weighted <- dfm_tfidf(subdfm)

#convert dfm's to dataframes
merged_df <- convert(subdfm, to = "data.frame")
merged_df_weighted <- convert(subdfm_weighted, to = "data.frame")

#drop document names: column 1
merged_df <- merged_df[,-1]
merged_df_weighted <- merged_df_weighted[,-1]
```

```{r}
#split back into train and test sets
dfTrain <- merged_df[which(merged$train_test == "train"), ]
dfTrain_w <- merged_df_weighted[which(merged$train_test == "train"), ]

dfTest <- merged_df[which(merged$train_test == "test"), ]
dfTest_w <- merged_df_weighted[which(merged$train_test == "test"), ]

#remove objects that are no longer necessary
remove(dtopwords, merged_dfm, merged_df, merged_df_weighted, subdfm, subdfm_weighted, topwords)

#append article labels as last column
dfTrain$author_gender <- merged$gender[which(merged$train_test == "train")]
dfTest$author_gender <- merged$gender[which(merged$train_test == "test")]
dfTrain_w$author_gender <- merged$gender[which(merged$train_test == "train")]
dfTest_w$author_gender <- merged$gender[which(merged$train_test == "test")]
```

#Model Training
##KNN
We use 5-fold cross validation to help reduce risk of overfitting in the models. 

```{r}
#resampling scheme
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE)

#fit a kNN model using the non-weighted Train set
#tuning parameter: K
set.seed(123)
knn.fit <- train(author_gender ~., 
                 data = dfTrain, 
                 method="knn", 
                 trControl = ctrl,
                 tuneLength = 20)

#fit kNN model using weighted Train set
#tuning parameter: K
set.seed(123)
knn.tfidf <- train(  author_gender ~., 
                 data = dfTrain_w, 
                 method="knn", 
                 trControl = ctrl,
                 tuneLength = 20)


#model output
knn.fit
knn.tfidf

plot(knn.fit) 

plot(knn.tfidf)

# predict on test data
knn.predict <- predict(knn.fit, newdata = dfTest)

knn.tfidf.predict <- predict(knn.tfidf, newdata = dfTest_w)

#confusion matrix
confusionMatrix(knn.predict, as.factor(dfTest$author_gender))

confusionMatrix(knn.tfidf.predict, as.factor(dfTest$author_gender))

#get execution time
knn.fit$times
knn.tfidf$times
```

## Random Forest 
```{r}
#set up parallel processing
#install.packages("doParallel")
library(doParallel)
getDoParWorkers()
cores <- 3
registerDoParallel(cores = cores)

#mtry: Number of random variables collected at each split
mtry <- sqrt(ncol(dfTrain))
#ntree: Number of trees to grow.
ntree <- 3

#randomly generate 15 mtry values with tuneLength = 15
control <- trainControl(method='cv', 
                        number=5, 
                        search = 'random')

set.seed(123)
rf.fit <- train(author_gender ~ .,
                   data = dfTrain,
                   method = 'rf',
                   metric = 'Accuracy',
                   tuneLength  = 15, 
                   trControl = control)

#repeat for weighted train set
set.seed(123)
rf.tfidf <- train(author_gender ~ .,
                   data = dfTrain_w,
                   method = 'rf',
                   metric = 'Accuracy',
                   tuneLength  = 15, 
                   trControl = control)




#output
rf.fit
rf.tfidf

#plots
plot(rf.fit)
plot(rf.tfidf)

#predict on test data
rf.predict <- predict(rf.fit,newdata = dfTest)

rf.tfidf.predict <- predict(rf.tfidf,newdata = dfTest_w)

#confusion matrix
confusionMatrix(rf.predict, as.factor(dfTest$author_gender))

confusionMatrix(rf.tfidf.predict, as.factor(dfTest$author_gender))

#compute time
rf.fit$times
rf.tfidf$times
```


## Support Vector Machine
```{r}
# resampling scheme
ctrl <- trainControl(method="cv", number = 5, classProbs = TRUE)

# fit SVM using the unweighted train set
# kernel: linear 
# tuning parameters: C 
set.seed(123)
svm.linear  <- train(author_gender ~ ., data=dfTrain, trControl = ctrl, method = "svmLinear")

# fit SVM using weighted train set
# kernel: linear 
# tuning parameters: C 
set.seed(123)
svm.tfidf.linear  <- train(author_gender ~ ., data=dfTrain_w, trControl = ctrl, method = "svmLinear")

#outputs
svm.linear
svm.tfidf.linear

# predict on test data
svm.linear.predict <- predict(svm.linear,newdata = dfTest)

svm.tfidf.linear.predict <- predict(svm.tfidf.linear,newdata = dfTest_w)

#confusion matrices
confusionMatrix(svm.linear.predict, as.factor(dfTest$author_gender)) #linear kernel, unweighted

confusionMatrix(svm.tfidf.linear.predict, as.factor(dfTest_w$author_gender)) #linear kernel, weighted


# get execution time
svm.linear$times
svm.tfidf.linear$times
```

##Naive Bayes
```{r, warning=FALSE, message=FALSE}
#resampling scheme
ctrl <- trainControl(method = "cv", number = 5)

#fit a Naive Bayes model using the unweighted Train set
set.seed(123)
nb.fit <- train(author_gender ~., 
                 data = dfTrain, 
                 method="nb", 
                 trControl = ctrl)

#fit a Naive Bayes model using weighted Train set
set.seed(123)
nb.tfidf <- train(author_gender ~., 
                 data = dfTrain_w, 
                 method="nb", 
                 trControl = ctrl)

#model output
nb.fit
nb.tfidf

plot(nb.fit) 

plot(nb.tfidf)

# predict on test data
nb.predict <- predict(nb.fit, newdata = dfTest)

nb.tfidf.predict <- predict(nb.tfidf, newdata = dfTest_w)

#confusion matrix
confusionMatrix(nb.predict, as.factor(dfTest$author_gender))

confusionMatrix(nb.tfidf.predict, as.factor(dfTest_w$author_gender))

#get execution time
nb.fit$times
nb.tfidf$times
```


##Ridge Regression
```{r}
# resampling scheme
ctrl <- trainControl("cv", number = 5)

#set up a grid range of lambda values
lambda <- 10^seq(-3, 3, length = 100)

#fit Ridge Regression using unweighted Train set
set.seed(123)
ridge <- train(
  author_gender ~., data = dfTrain, method = "glmnet",
  trControl = ctrl,
  tuneGrid = expand.grid(alpha = 0, lambda = lambda)
  )

#fit Ridge Regression using unweighted Train set
set.seed(123)
ridge.tfidf <- train(
  author_gender ~., data = dfTrain_w, method = "glmnet",
  trControl = ctrl,
  tuneGrid = expand.grid(alpha = 0, lambda = lambda)
  )


#model output 
ridge
ridge.tfidf

# predict on test data
ridge.predict <- predict(ridge, newdata = dfTest) 

ridge.tfidf.predict <- predict(ridge.tfidf, newdata = dfTest_w)

#confusion matrix
confusionMatrix(ridge.predict, as.factor(dfTest$author_gender))

confusionMatrix(ridge.tfidf.predict, as.factor(dfTest_w$author_gender))

#get execution time
ridge$times
ridge.tfidf$times
```


```{r}
models <- list(Ridge = ridge, Ridge.tfidf = ridge.tfidf, NaiveBayes = nb.fit, NaiveBayes.tfidf = nb.tfidf, RandomForest = rf.fit, RandomForest.tfidf = rf.tfidf, SVM.linear = svm.linear, SVM.tdidf.linear = svm.tfidf.linear, kNN = knn.fit, kNN.tfidf = knn.tfidf)
results <- resamples(models)

#boxplot
bwplot(results)

# stopCluster(cores)
```